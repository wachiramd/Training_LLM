# Useful resources

Here are some helpful resources I used while making this course. I’m sharing them with you in case they help. Enjoy!

## Videos

- [Deep Dive into LLMs like ChatGPT (Theory)](https://www.youtube.com/watch?v=7xTGNNLPyMI&t=7652s)
- [Building GPT from Scratch (Practice 1)](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1s)
- [Building a GPT Tokenizer (Practice 2)](https://www.youtube.com/watch?v=zduSFxRajkE&t=440s)
- [Reproducing GPT-2 (Practice 3)](https://www.youtube.com/watch?v=l8pRSuU81PU)

## GitHub repositories

- [Train a language model using WhatsApp group chats](https://github.com/bernhard-pfann/lad-gpt)
- [Create an AI version of yourself using WhatsApp chats](https://github.com/kinggongzilla/ai-clone-whatsapp)
- [Extract WhatsApp key/database without root access](https://github.com/YuvrajRaghuvanshiS/WhatsApp-Key-Database-Extractor)

## Articles

- [Train a language model on your WhatsApp chats](https://towardsdatascience.com/build-a-language-model-on-your-whatsapp-chats-31264a9ced90/)
- [Fine-tune an LLM to create your digital twin](https://medium.com/better-programming/unleash-your-digital-twin-how-fine-tuning-llm-can-create-your-perfect-doppelganger-b5913e7dda2e)
- [Fine-tuning LLMs using QLoRA](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
- [Guide to fine-tuning large language models](https://www.datacamp.com/tutorial/fine-tuning-large-language-models)
- [Understanding LoRA and DoRA for model tuning](https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch)
- [Step-by-step guide to fine-tune LLMs with LoRA](https://medium.com/@manindersingh120996/practical-guide-to-fine-tune-llms-with-lora-c835a99d7593)
- [Fine-tuning LLMs for multi-turn conversations](https://www.together.ai/blog/fine-tuning-llms-for-multi-turn-conversations-a-technical-deep-dive#:~:text=Fine-tuning%20LLMs%20for%20multi-turn%20conversations%20requires%20careful%20attention,while%20managing%20computational%20resources%20efficiently.)
- [Fine-tuning mixtral 7bx8 with LoRA](https://medium.com/@prakharsaxena11111/finetuning-mixtral-7bx8-6071b0ebf114)
- [Positional Encoding Explained: A Deep Dive into Transformer PE](https://medium.com/thedeephub/positional-encoding-explained-a-deep-dive-into-transformer-pe-65cfe8cfe10b)
- [You could have designed state of the art positional encoding](https://huggingface.co/blog/designing-positional-encoding)
- [Relative Positional Encoding](https://jaketae.github.io/study/relative-positional-encoding/)
- [What is grouped query attention (GQA)?](https://www.ibm.com/think/topics/grouped-query-attention)
- [Linear Attention Is All You Need](https://medium.com/data-science/linear-attention-is-all-you-need-5fa9c845c1b5)
- [Attention Variations — MQA vs GQA vs MHA vs MLA](https://verticalserve.medium.com/group-query-attention-58283b337c65)
- [Understanding Multi-Head Latent Attention](https://planetbanatt.net/articles/mla.html)
- [DeepSeek's Multi-Head Latent Attention](https://liorsinai.github.io/machine-learning/2025/02/22/mla.html#multi-head-latent-attention)
- [Sliding Window Attention](https://medium.com/@manojkumal/sliding-window-attention-565f963a1ffd)
- [Can LLMs learn from a single example?](https://www.fast.ai/posts/2023-09-04-learning-jumps/)
- [Normalization Layer Placement (Pre-LN vs Post-LN)](https://apxml.com/courses/how-to-build-a-large-language-model/chapter-11-scaling-transformers-architectural-choices/normalization-layer-placement)
- [Batch Normalization, Layer Normalization and Root Mean Square Layer Normalization: A Comprehensive Guide with Python Implementations](https://afterhoursresearch.hashnode.dev/batch-normalization-layer-normalization-and-root-mean-square-layer-normalization-a-comprehensive-guide-with-python-implementations)
- [Deep Dive into Deep Learning: Layers, RMSNorm, and Batch Normalization](https://2020machinelearning.medium.com/deep-dive-into-deep-learning-layers-rmsnorm-and-batch-normalization-b2423552be9f)
- [Exploring SwiGLU : The Activation Function Powering Modern LLMs](https://medium.com/@s_boudefel/exploring-swiglu-the-activation-function-powering-modern-llms-9697f88221e7)
- [All the Activation Functions (and a history of deep learning)](https://dublog.net/blog/all-the-activations/)
- [Understanding and Using Supervised Fine-Tuning (SFT) for Language Models)](https://cameronrwolfe.substack.com/p/understanding-and-using-supervised)

## Reddit discussions

- [Finetuned Llama 2-7B using WhatsApp chats](https://www.reddit.com/r/LocalLLaMA/comments/18ny05c/finetuned_llama_27b_on_my_whatsapp_chats/)
- [How to train your model](https://www.reddit.com/r/Oobabooga/comments/19480dr/how_to_train_your_dra_model/?share_id=FandRNmK84MItOJYIynap&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1)
- [Exporting full WhatsApp chat history](https://www.reddit.com/r/DataHoarder/comments/a7c0yq/full_whatsapp_chat_export_40000_messages/)
- [Normalization in transformers](https://www.reddit.com/r/MachineLearning/comments/1ecict8/d_normalization_in_transformers/)

## Notebooks

- [Unsloth AI Notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks)
- [LLMs-from-Scratch: Chapter 6 Notebook](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb?utm_source=substack&utm_medium=email)
- [LoRA Implementation from Scratch](https://www.kaggle.com/code/aisuko/lora-from-scratch)

## Scripts

- [Multi-Head Latent Attention (MLA) Implementation](https://github.com/ambisinister/mla-experiments/blob/main/modeling/attention/mla.py)
- [RMSNorm Implementation](https://github.com/meta-llama/llama/blob/main/llama/model.py#L34-L77)

## Research papers

- [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/pdf/2405.04434)
- [Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention](https://arxiv.org/pdf/2006.16236)
- [LINEAR ATTENTION IS (MAYBE) ALL YOU NEED (TO UNDERSTAND TRANSFORMER OPTIMIZATION)](https://arxiv.org/pdf/2310.01082)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
- [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150)
- [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025)
- [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://arxiv.org/pdf/2305.13245)
- [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062)
- [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467)
- [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
- [On Layer Normalization in the Transformer Architecture](https://arxiv.org/pdf/2002.04745)
- [Root Mean Square Layer Normalization](https://arxiv.org/pdf/1910.07467)
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167)
- [Layer Normalization](https://arxiv.org/pdf/1607.06450)
- [Dropout Reduces Underfitting](https://arxiv.org/pdf/2303.01500)
- [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202v1)
- [The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities (Version 1.0)](https://arxiv.org/html/2408.13296v1)
